{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Management for Data Quality\n",
    "**Description**: Store and use metadata to manage data quality in a pipeline.\n",
    "\n",
    "**Steps**:\n",
    "1. Load metadata\n",
    "2. Load data\n",
    "3. Use metadata to validate data quality\n",
    "4. Show valid data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Data:\n",
      "   customer_id  amount        date\n",
      "0            1   200.0  2023-01-01\n",
      "1            2   150.0  2023-01-02\n",
      "2            3     NaN  2023-01-03\n",
      "3            4   300.0  2023-01-04\n",
      "4            5   250.0  2023-01-05\n",
      "5            2   150.0  2023-01-06\n",
      "6            6   500.0  2023-01-07\n",
      "7            3   100.0  2023-01-08\n",
      "\n",
      "Metadata:\n",
      "{'file': 'generated_in_notebook', 'row_count': 8, 'columns': ['customer_id', 'amount', 'date'], 'column_types': {'customer_id': 'int64', 'amount': 'float64', 'date': 'object'}, 'last_updated': '2025-05-28 10:25:21.544388', 'transformations': []}\n",
      "\n",
      "Data Quality Issues:\n",
      "Column 'amount' has 1 missing values.\n",
      "\n",
      "Cleaned data saved to 'cleaned_data.csv'\n",
      "\n",
      "Final Cleaned Data:\n",
      "   customer_id      amount       date\n",
      "0            1  200.000000 2023-01-01\n",
      "1            2  150.000000 2023-01-02\n",
      "2            3  235.714286 2023-01-03\n",
      "3            4  300.000000 2023-01-04\n",
      "4            5  250.000000 2023-01-05\n",
      "6            6  500.000000 2023-01-07\n",
      "\n",
      "Data passed all quality checks!\n",
      "\n",
      "Data Quality Issues:\n",
      "Column 'amount' has 1 missing values.\n",
      "\n",
      "Data Quality Issues:\n",
      "Column 'amount' has 1 missing values.\n",
      "Column 'date' contains 1 invalid dates.\n",
      "\n",
      "Unit Tests Passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12459/2306376507.py:133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_invalid['amount'].iloc[1] = np.nan\n",
      "/tmp/ipykernel_12459/2306376507.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_invalid['date'].iloc[1] = 'invalid_date'\n",
      "/tmp/ipykernel_12459/2306376507.py:137: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'invalid_date' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
      "  df_invalid['date'].iloc[1] = 'invalid_date'\n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Create the dataset directly within the code\n",
    "data = {\n",
    "    'customer_id': [1, 2, 3, 4, 5, 2, 6, 3],\n",
    "    'amount': [200, 150, np.nan, 300, 250, 150, 500, 100],\n",
    "    'date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05', '2023-01-06', '2023-01-07', '2023-01-08']\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Metadata structure\n",
    "metadata = {\n",
    "    'file': 'generated_in_notebook',\n",
    "    'row_count': len(df),\n",
    "    'columns': list(df.columns),\n",
    "    'column_types': {col: str(df[col].dtype) for col in df.columns},\n",
    "    'last_updated': str(datetime.now()),\n",
    "    'transformations': []\n",
    "}\n",
    "\n",
    "# Display the dataset and metadata\n",
    "print(\"Loaded Data:\")\n",
    "print(df)\n",
    "print(\"\\nMetadata:\")\n",
    "print(metadata)\n",
    "\n",
    "# Step 3: Function to validate data quality based on metadata\n",
    "def validate_data_quality(df, metadata):\n",
    "    \"\"\"\n",
    "    Check for missing values, invalid data types, and other quality checks.\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    for col in missing_values.index:\n",
    "        if missing_values[col] > 0:\n",
    "            issues.append(f\"Column '{col}' has {missing_values[col]} missing values.\")\n",
    "    \n",
    "    # Check for data type validity\n",
    "    for col in df.columns:\n",
    "        expected_type = metadata['column_types'].get(col)\n",
    "        if expected_type and not np.issubdtype(df[col].dtype, np.generic):\n",
    "            issues.append(f\"Column '{col}' has an unexpected data type. Expected {expected_type}, got {str(df[col].dtype)}\")\n",
    "\n",
    "    # Check if 'date' column has a valid date format\n",
    "    try:\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Invalid date values turn into NaT\n",
    "        invalid_dates = df['date'].isnull().sum()\n",
    "        if invalid_dates > 0:\n",
    "            issues.append(f\"Column 'date' contains {invalid_dates} invalid dates.\")\n",
    "    except Exception as e:\n",
    "        issues.append(f\"Error parsing 'date' column: {str(e)}\")\n",
    "    \n",
    "    # Print issues if any, else indicate no issues\n",
    "    if issues:\n",
    "        print(\"\\nData Quality Issues:\")\n",
    "        for issue in issues:\n",
    "            print(issue)\n",
    "    else:\n",
    "        print(\"\\nData passed all quality checks!\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Validate the data quality\n",
    "data_quality_issues = validate_data_quality(df, metadata)\n",
    "\n",
    "# Step 4: Log transformations dynamically\n",
    "def log_transformation(metadata, transformation_details):\n",
    "    \"\"\"\n",
    "    Add details of data transformations to the metadata.\n",
    "    \"\"\"\n",
    "    transformation_entry = {\n",
    "        'transformation': transformation_details,\n",
    "        'timestamp': str(datetime.now())\n",
    "    }\n",
    "    metadata['transformations'].append(transformation_entry)\n",
    "\n",
    "# Example transformations\n",
    "log_transformation(metadata, \"Imputed missing values in 'amount' column with mean.\")\n",
    "log_transformation(metadata, \"Removed duplicates based on 'customer_id' column.\")\n",
    "\n",
    "# Step 5: Apply data cleaning or transformations and log those\n",
    "# Filling missing values in 'amount' column with the mean\n",
    "df['amount'].fillna(df['amount'].mean(), inplace=True)  # Example transformation\n",
    "log_transformation(metadata, \"Imputed missing 'amount' values using mean.\")\n",
    "\n",
    "# Remove duplicates based on customer_id\n",
    "df.drop_duplicates(subset='customer_id', keep='first', inplace=True)\n",
    "log_transformation(metadata, \"Removed duplicate rows based on 'customer_id'.\")\n",
    "\n",
    "# Step 6: Save metadata to a JSON file for future tracking\n",
    "def save_metadata(metadata, metadata_path='metadata.json'):\n",
    "    \"\"\"\n",
    "    Save metadata to a JSON file for tracking.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving metadata: {e}\")\n",
    "\n",
    "# Save the updated metadata\n",
    "save_metadata(metadata)\n",
    "\n",
    "# Step 7: Save cleaned data to a new file (simulating saving to CSV)\n",
    "df.to_csv(\"cleaned_data.csv\", index=False)\n",
    "print(\"\\nCleaned data saved to 'cleaned_data.csv'\")\n",
    "\n",
    "# Final metadata and data output\n",
    "print(\"\\nFinal Cleaned Data:\")\n",
    "print(df)\n",
    "\n",
    "# Unit Test Functions (to check validation, cleaning, and transformation)\n",
    "def test_validate_data_quality():\n",
    "    # Test valid data\n",
    "    df_valid = pd.DataFrame({\n",
    "        'customer_id': [1, 2, 3],\n",
    "        'amount': [100, 200, 300],\n",
    "        'date': ['2023-01-01', '2023-01-02', '2023-01-03']\n",
    "    })\n",
    "    metadata_valid = {'columns': df_valid.columns.tolist(), 'column_types': {col: str(df_valid[col].dtype) for col in df_valid.columns}}\n",
    "    assert len(validate_data_quality(df_valid, metadata_valid)) == 0  # Should pass with no issues\n",
    "\n",
    "    # Test data with missing value in 'amount' column\n",
    "    df_invalid = df_valid.copy()\n",
    "    df_invalid['amount'].iloc[1] = np.nan\n",
    "    assert len(validate_data_quality(df_invalid, metadata_valid)) > 0  # Should flag missing value issue\n",
    "\n",
    "    # Test invalid date format\n",
    "    df_invalid['date'].iloc[1] = 'invalid_date'\n",
    "    assert len(validate_data_quality(df_invalid, metadata_valid)) > 0  # Should flag invalid date issue\n",
    "\n",
    "\n",
    "# Run the unit tests\n",
    "test_validate_data_quality()\n",
    "print(\"\\nUnit Tests Passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
